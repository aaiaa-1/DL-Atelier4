# Deep Learning Architectures: Autoencoders, Variational Autoencoders, and Generative Adversarial Networks

Welcome to the Deep Learning Architectures repository! This project is designed to provide a comprehensive understanding of and practical experience with building and training deep neural networks using PyTorch. Our focus lies on three fundamental architectures: Autoencoders (AE), Variational Autoencoders (VAE), and Generative Adversarial Networks (GANs).

## Part 1: Autoencoders and Variational Autoencoders
In this section, we delve into the world of Autoencoders and their probabilistic counterpart, Variational Autoencoders. Here's what you'll find:

1. **Autoencoders (AE)**: Construct and train an autoencoder architecture on the MNIST dataset. Explore how the model learns to reconstruct input images while compressing them into a latent space representation.
   
2. **Variational Autoencoders (VAE)**: Develop and train a variational autoencoder architecture on the MNIST dataset. Dive into the probabilistic nature of VAEs and understand how they enable more robust generative capabilities.

3. **Evaluation and Analysis**: Dive deep into evaluating and analyzing both AE and VAE models. Explore metrics such as loss functions and Kullbackâ€“Leibler (KL) divergence to gain insights into model performance.

4. **Latent Space Visualization**: Visualize the latent spaces learned by the AE and VAE models. Gain intuition about the distribution of encoded representations and their impact on generative capabilities.

- **MNIST Dataset for Autoencoders and Variational Autoencoders:** [MNIST Dataset](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)

## Part 2: Generative Adversarial Networks
Moving on to Generative Adversarial Networks (GANs), we explore the fascinating realm of generating new data samples that mimic the characteristics of the training data. Here's what this section offers:

1. **GAN Architectures**: Set up and train Generative Adversarial Network architectures using the Abstract Art Gallery dataset. Delve into the intricate interplay between the Generator and Discriminator networks.

2. **Performance Analysis**: Assess the performance of both the Generator and Discriminator networks. Understand the nuances of training GANs and how different architectural choices impact the quality of generated samples.

3. **Data Generation and Quality Assessment**: Generate novel data samples using trained GAN models and evaluate their quality in comparison to the original dataset. Explore techniques for improving the fidelity of generated samples.

- **Abstract Art Gallery Dataset for Generative Adversarial Networks:** [Abstract Art Gallery Dataset](https://www.kaggle.com/datasets/bryanb/abstract-art-gallery)

We hope this repository enhances your understanding of deep learning architectures and inspires you to explore further in this exciting field!
# DL-Atelier4
